# ---------------------------------------------------------------
# The calculation of instance_loss, CBS, BPS, dual-level interaction are proposed by
# Anonymous ICME2023 Authors, Paper-id:541
# ---------------------------------------------------------------

# The ema model update and the domain-mixing are based on:
# https://github.com/vikolss/DACS
# Copyright (c) 2020 vikolss. Licensed under the MIT License.
# A copy of the license is available at resources/license_dacs

# The Rare Class Sampling and ImageNet Feature distance are based in:
# https://github.com/lhoyer/DAFormer
# Copyright (c) 2021-2022 ETH Zurich, Lukas Hoyer. All rights reserved.
# Licensed under the Apache License, Version 2.0

from asyncio import base_tasks
import math
import os
import random
from copy import deepcopy

import mmcv
import numpy as np
import torch
from matplotlib import pyplot as plt
from timm.models.layers import DropPath
from torch.nn.modules.dropout import _DropoutNd
import torch.nn.functional as F

from mmseg.core import add_prefix
from mmseg.models import UDA, build_segmentor
from mmseg.models.uda.uda_decorator import UDADecorator, get_module
from mmseg.models.utils.dacs_transforms import (denorm, gaussian_blur_tar, color_jitter_tar, get_class_masks,
                                                get_mean_std, strong_transform)
from mmseg.models.utils.visualization import subplotimg
from mmseg.utils.utils import downscale_label_ratio

# pdb to debug
import pdb

# gc to clean memory
import gc
# einops package
from einops import rearrange, reduce, repeat
from einops.layers.torch import Rearrange, Reduce

# csv module
import csv

# pandas module
import pandas as pd

# random module
import random

# sklearn
from sklearn.cluster import KMeans

def _params_equal(ema_model, model):
    for ema_param, param in zip(ema_model.named_parameters(),
                                model.named_parameters()):
        if not torch.equal(ema_param[1].data, param[1].data):
            # print("Difference in", ema_param[0])
            return False
    return True


def calc_grad_magnitude(grads, norm_type=2.0):
    norm_type = float(norm_type)
    if norm_type == math.inf:
        norm = max(p.abs().max() for p in grads)
    else:
        norm = torch.norm(
            torch.stack([torch.norm(p, norm_type) for p in grads]), norm_type)

    return norm


@UDA.register_module()
class DACS(UDADecorator):

    def __init__(self, **cfg):
        super(DACS, self).__init__(**cfg)
        self.local_iter = 0
        self.max_iters = cfg['max_iters']
        self.alpha = cfg['alpha']
        self.pseudo_threshold = cfg['pseudo_threshold']
        self.psweight_ignore_top = cfg['pseudo_weight_ignore_top']
        self.psweight_ignore_bottom = cfg['pseudo_weight_ignore_bottom']
        self.fdist_lambda = cfg['imnet_feature_dist_lambda']
        self.fdist_classes = cfg['imnet_feature_dist_classes']
        self.fdist_scale_min_ratio = cfg['imnet_feature_dist_scale_min_ratio']
        self.enable_fdist = self.fdist_lambda > 0
        self.mix = cfg['mix']
        self.blur = cfg['blur']
        self.color_jitter_s = cfg['color_jitter_strength']
        self.color_jitter_p = cfg['color_jitter_probability']
        self.debug_img_interval = cfg['debug_img_interval']
        self.print_grad_magnitude = cfg['print_grad_magnitude']
        assert self.mix == 'class'

        self.debug_fdist_mask = None
        self.debug_gt_rescale = None

        self.class_probs = {}
        ema_cfg = deepcopy(cfg['model'])
        self.ema_model = build_segmentor(ema_cfg)

        if self.enable_fdist:
            self.imnet_model = build_segmentor(deepcopy(cfg['model']))
        else:
            self.imnet_model = None
        
        # Create a feature and a label memory buffer
        self.buffer_size = 300 # can be viewed as a hyperparameter
        dim = 256   

        self.dataset = 1 # 0 for gta2cs, 1 for synthia2cs

        # load saved buffer
        if self.buffer_size == 50:
            buffer_path = './embedding_cache/balanced avg cache 50'
        elif self.buffer_size == 100:
            buffer_path = './embedding_cache/balanced avg cache 100'
        elif self.buffer_size == 200:
            buffer_path = './embedding_cache/balanced avg cache 200'
        elif self.buffer_size == 300:
            buffer_path = './embedding_cache/balanced avg cache 300'
        elif self.buffer_size == 500:
            buffer_path = './embedding_cache/balanced avg cache 500'
        
        if self.dataset:
            buffer_path = './embedding_cache/Synthia2cs cache 300'

        fb_path = os.path.join(buffer_path, 'f_buffer_s.npz')
        lb_path = os.path.join(buffer_path, 'l_buffer_s.npz')
        self.f_buffer = torch.tensor(np.load(fb_path)['arr_0'], dtype=torch.float32)
        self.l_buffer = torch.tensor(np.load(lb_path)['arr_0'], dtype=torch.long) # label buffer
        
        "if load full buffer, set buffer_idx = buffer_size"
        self.buffer_idx = self.buffer_size

        "hyper parameters for further update buffer"
        self.update_idx = 0
        self.if_update = 0
        self.update_interval = 50
        self.momentum = 0.999
        self.BPS_th = 2 # threshold for Boundary Pixel Selecting 
        self.update_stride = 5
        self.stop_update = 50

        "hyper parameters for calibrarion"
        self.if_calibrate = 1
        # set softmax temperature w(eak)t and s(trong)t
        self.wt = 1
        self.st = 1
        # set smooth parameter Î±: aggregate smooth
        self.a_smooth = 1

        "hyper parameters instance loss"
        self.ins_lambda = 0.5



    def get_ema_model(self):
        return get_module(self.ema_model)

    def get_imnet_model(self):
        return get_module(self.imnet_model)

    def _init_ema_weights(self):
        for param in self.get_ema_model().parameters():
            param.detach_()
        mp = list(self.get_model().parameters())
        mcp = list(self.get_ema_model().parameters())
        for i in range(0, len(mp)):
            if not mcp[i].data.shape:  # scalar tensor
                mcp[i].data = mp[i].data.clone()
            else:
                mcp[i].data[:] = mp[i].data[:].clone()

    def _update_ema(self, iter):
        alpha_teacher = min(1 - 1 / (iter + 1), self.alpha)
        for ema_param, param in zip(self.get_ema_model().parameters(),
                                    self.get_model().parameters()):
            if not param.data.shape:  # scalar tensor
                ema_param.data = \
                    alpha_teacher * ema_param.data + \
                    (1 - alpha_teacher) * param.data
            else:
                ema_param.data[:] = \
                    alpha_teacher * ema_param[:].data[:] + \
                    (1 - alpha_teacher) * param[:].data[:]

    def train_step(self, data_batch, optimizer, **kwargs):
        """The iteration step during training.

        This method defines an iteration step during training, except for the
        back propagation and optimizer updating, which are done in an optimizer
        hook. Note that in some complicated cases or models, the whole process
        including back propagation and optimizer updating is also defined in
        this method, such as GAN.

        Args:
            data (dict): The output of dataloader.
            optimizer (:obj:`torch.optim.Optimizer` | dict): The optimizer of
                runner is passed to ``train_step()``. This argument is unused
                and reserved.

        Returns:
            dict: It should contain at least 3 keys: ``loss``, ``log_vars``,
                ``num_samples``.
                ``loss`` is a tensor for back propagation, which can be a
                weighted sum of multiple losses.
                ``log_vars`` contains all the variables to be sent to the
                logger.
                ``num_samples`` indicates the batch size (when the model is
                DDP, it means the batch size on each GPU), which is used for
                averaging the logs.
        """

        optimizer.zero_grad()
        log_vars = self(**data_batch)
        optimizer.step()

        log_vars.pop('loss', None)  # remove the unnecessary 'loss'
        outputs = dict(
            log_vars=log_vars, num_samples=len(data_batch['img_metas']))
        return outputs

    def masked_feat_dist(self, f1, f2, mask=None):
        feat_diff = f1 - f2
        # mmcv.print_log(f'fdiff: {feat_diff.shape}', 'mmseg')
        pw_feat_dist = torch.norm(feat_diff, dim=1, p=2)
        # mmcv.print_log(f'pw_fdist: {pw_feat_dist.shape}', 'mmseg')
        if mask is not None:
            # mmcv.print_log(f'fd mask: {mask.shape}', 'mmseg')
            pw_feat_dist = pw_feat_dist[mask.squeeze(1)]
            # mmcv.print_log(f'fd masked: {pw_feat_dist.shape}', 'mmseg')
        return torch.mean(pw_feat_dist)

    def calc_feat_dist(self, img, gt, feat=None):
        assert self.enable_fdist
        with torch.no_grad():
            self.get_imnet_model().eval()
            feat_imnet = self.get_imnet_model().extract_feat(img)
            feat_imnet = [f.detach() for f in feat_imnet]
        lay = -1
        if self.fdist_classes is not None:
            fdclasses = torch.tensor(self.fdist_classes, device=gt.device)
            scale_factor = gt.shape[-1] // feat[lay].shape[-1]
            gt_rescaled = downscale_label_ratio(gt, scale_factor,
                                                self.fdist_scale_min_ratio,
                                                self.num_classes,
                                                255).long().detach()
            fdist_mask = torch.any(gt_rescaled[..., None] == fdclasses, -1)
            feat_dist = self.masked_feat_dist(feat[lay], feat_imnet[lay],
                                              fdist_mask)
            self.debug_fdist_mask = fdist_mask
            self.debug_gt_rescale = gt_rescaled
        else:
            feat_dist = self.masked_feat_dist(feat[lay], feat_imnet[lay])
        feat_dist = self.fdist_lambda * feat_dist
        feat_loss, feat_log = self._parse_losses(
            {'loss_imnet_feat_dist': feat_dist})
        feat_log.pop('loss', None)
        return feat_loss, feat_log

    @torch.no_grad()
    def instance_feature_avg(self, feat, labels, device):
        """Averaging the x_feat to 19 instance-features
        and update the memory buffer"""
        # Direct return when buffer is full and requires no update
        if self.buffer_idx >=  self.buffer_size and self.if_update == 0:
            return
        # count update_stride; defalut as < len(rare_seq)
        cnt_step = 0
        batch_size1 = feat.shape[0] # batch size
        # channel_size = feat.shape[1] # channel size
        # flatten feature HxW to 1 dim
        feat_flt = rearrange(feat, 'b c h w -> b c (h w)')    # feat_flt.shape: [2, 256, 512*512]
        label_flt = rearrange(labels, 'b c h w -> b c (h w)') # label_flt.shape: [2, 1, 512*512]
        # get index of each class
        # class_num = 19
        # rare class sequence
        rare_seq = [18, 12, 17, 16, 7, 6, 11, 15, 4, 5, 14, 3, 9, 13, 8, 1, 10, 2, 0]
        # extract feature and update buffer 
        # take one image at a time
        for k in range(batch_size1): # normally batch is 2
            # averaging instance(class), with i in rare class sequence
            for i in rare_seq:  
                srch = (label_flt==i).nonzero(as_tuple=False)   # srch: search vector locations with class i
                idx = torch.zeros(srch.shape[0],  dtype=torch.int32, device=device) # remember to add dtype and device
                for j in range(srch.shape[0]):
                    idx[j] = srch[j][2]  # flattened locations at dim=2
                # feature map for batch k, shape[256, 512*512] with channel and index. 
                feat_tmp = feat_flt[k]  
                # select channels with given index: feat_select
                feat_select = torch.index_select(feat_tmp, dim = 1, index = idx) 
                feat_mean = reduce(feat_select, 'a b -> a', reduction='mean')   # take average and normalize to 1 dimension
                # store into memory buffer
                if len(idx) == 0: # no matching class for i
                    continue
                else:
                    if self.if_update == 0: # normally update buffer
                        self.update_buffer(feat_mean, i)
                    if self.if_update == 1: # allow further update
                        unequal_seq =self.is_equally_distrubuted()
                        # if there is no unequal class, update as step
                        if len(unequal_seq) == 0:
                            self.update_buffer_further(feat_mean, i)
                            cnt_step += 1
                            if cnt_step == self.update_stride:
                                return
                        # if there are missing classes, update as full rare seq
                        else:
                            break

            """If there are unequal classes during an update period,
            fill the unequal blank"""
            if len(self.is_equally_distrubuted()) > 0 and self.if_update == 1:
                for i in unequal_seq:  
                    srch = (label_flt==i).nonzero(as_tuple=False)   # srch: search vector locations with class i
                    idx = torch.zeros(srch.shape[0],  dtype=torch.int32, device=device) # remember to add dtype and device
                    for j in range(srch.shape[0]):
                        idx[j] = srch[j][2]  # flattened locations at dim=2
                    # feature map for batch k, shape[256, 512*512] with channel and index. 
                    feat_tmp = feat_flt[k]  
                    # select channels with given index: feat_select
                    feat_select = torch.index_select(feat_tmp, dim = 1, index = idx) 
                    feat_mean = reduce(feat_select, 'a b -> a', reduction='mean')   # take average and normalize to 1 dimension
                    # store into memory buffer
                    if len(idx) == 0: # no matching class for i
                        continue
                    else:
                        self.update_buffer_further(feat_mean, i)

    @torch.no_grad()
    def ema_update_bank(self, feat, labels, device):
        """ ema update the memory buffer with CBS and BPS&K-means Clustering"""
        # Direct return when buffer is full and requires no update
        if self.buffer_idx >=  self.buffer_size and self.if_update == 0:
            return
        # count update_stride; defalut as < len(rare_seq)
        cnt_step = 0
        batch_size1 = feat.shape[0] # batch size
        # channel_size = feat.shape[1] # channel size
        # flatten feature HxW to 1 dim
        feat_flt = rearrange(feat, 'b c h w -> b c (h w)')    # feat_flt.shape: [2, 256, 512*512]
        label_flt = rearrange(labels, 'b c h w -> b c (h w)') # label_flt.shape: [2, 1, 512*512]
        # get index of each class
        class_num = 19
        # channel size
        channel_size = 256
        # rare class sequence
        rare_seq = [18, 12, 17, 16, 7, 6, 11, 15, 4, 5, 14, 3, 9, 13, 8, 1, 10, 2, 0]
        # extract feature and update buffer 
        # take one image at a time
        for k in range(batch_size1): # normally batch is 2
            # only update with 1 image
            if k == 1:
                break
            # averaging instance(class), with i in rare class sequence
            # initialize a temporary cache for the 19 instances, channels = 256
            cls_cache = torch.zeros(class_num, channel_size,  dtype=torch.float32, device=device) # mind the dtype!
            for i in rare_seq:  
                srch = (label_flt==i).nonzero(as_tuple=False)   # srch: search vector locations with class i
                # search for boundary pixels
                label_flt_s = label_flt.squeeze(1)
                # define receptive field operation matrix
                img_width = 512
                op_seq = [-1-img_width, -img_width, 1-img_width,
                            -1,         0,          1, 
                        -1+img_width, img_width, 1+img_width]
                # k means indx
                idx_all = torch.zeros(srch.shape[0],  dtype=torch.int32, device=device) # remember to add dtype and device
                
                idx = [] # index sequence for boundary search
                for K in range(srch.shape[0]):
                    j = random.randint(0, srch.shape[0]-1)
                    # flattened search locations at dim=2
                    cls_cnt = []
                    center_idx = srch[j][2] 
                    idx_all[K] = center_idx
                    for op in op_seq:
                        rec_idx = center_idx + op   # search labels within the receptive field
                        if rec_idx >= 0 and rec_idx < label_flt.shape[2]: # if receptive idx valid
                            r_label = label_flt_s[k][rec_idx]
                            if r_label not in cls_cnt:
                                cls_cnt.append(r_label) # enqueue labels 
                    
                    if len(cls_cnt) >= self.BPS_th: # if at boundary
                        idx.append(center_idx)
                        break       # randomly select one valid pixel and continue
                
                idx = torch.tensor(idx, device=device) # list to tensor
                # feature map for batch k, shape[256, 512*512] with channel and index. 
                feat_tmp = feat_flt[k]  
                # boundary pixel selecting
                feat_BPS = torch.index_select(feat_tmp, dim = 1, index = idx) 
                # all pixels selected
                feat_select = torch.index_select(feat_tmp, dim = 1, index = idx_all)  
                
                feat_select = rearrange(feat_select, 'c n -> n c')
                # 1. take average and normalize to 1 dimension
                # 2. K-means aggregation to select 1 representative feature
                # 3. supervised select boundary pixels
                feat_mean = reduce(feat_BPS, 'a b -> a', reduction='mean')
                # store into memory buffer
                if len(idx_all) == 0: # no matching class for i
                    continue
                else:
                    n_clusters=1
                    feat_select = feat_select.cpu()
                    cluster = KMeans(n_clusters=n_clusters, random_state=0).fit(feat_select)   
                    feat_center = cluster.cluster_centers_
                    feat_center = feat_center.squeeze(0) # feat_center [1, 256] numpy arrays on cpu
                    # np.arrays to tensor and device
                    feat_center = torch.as_tensor(feat_center, dtype = torch.float32, device=device)
                    
                    cls_cache[i] = (feat_center + feat_mean) /2

                    # cls_cache[i] = feat_mean
            # after storing features in cls_cache [2, 19, 256]
            # Ema update to buffer
            self.l_buffer = self.l_buffer.to(device)
            self.f_buffer = self.f_buffer.to(device)
            self.l_buffer.detach()
            self.f_buffer.detach()

            label_expand = self.l_buffer.expand(256, self.buffer_size)
            label_expand = rearrange(label_expand, 'c n-> n c') # reshape to [K, 256] 
            cash_in = cls_cache.gather(0, label_expand) # cache_in [k, 256]
            # ema update buffer
            self.f_buffer = self.momentum * self.f_buffer + (1-self.momentum) * cash_in

            self.l_buffer = self.l_buffer.to('cpu')
            self.f_buffer = self.f_buffer.to('cpu')

    @torch.no_grad()
    def ema_update_bank_CBS_only(self, feat, labels, device):
        """ ema update the memory buffer with CBS only"""
        # Direct return when buffer is full and requires no update
        if self.buffer_idx >=  self.buffer_size and self.if_update == 0:
            return
        # count update_stride; defalut as < len(rare_seq)
        cnt_step = 0
        batch_size1 = feat.shape[0] # batch size
        # channel_size = feat.shape[1] # channel size
        # flatten feature HxW to 1 dim
        feat_flt = rearrange(feat, 'b c h w -> b c (h w)')    # feat_flt.shape: [2, 256, 512*512]
        label_flt = rearrange(labels, 'b c h w -> b c (h w)') # label_flt.shape: [2, 1, 512*512]
        # get index of each class
        class_num = 19
        # channel size
        channel_size = 256
        # rare class sequence
        rare_seq = [18, 12, 17, 16, 7, 6, 11, 15, 4, 5, 14, 3, 9, 13, 8, 1, 10, 2, 0]
        # extract feature and update buffer 
        # take one image at a time
        for k in range(batch_size1): # normally batch is 2
            # only update with 1 image
            if k == 1:
                break
            # averaging instance(class), with i in rare class sequence
            # initialize a temporary cache for the 19 instances, channels = 256
            cls_cache = torch.zeros(class_num, channel_size,  dtype=torch.float32, device=device) # mind the dtype!
            for i in rare_seq:  
                srch = (label_flt==i).nonzero(as_tuple=False)   # srch: search vector locations with class i
                idx = torch.zeros(srch.shape[0],  dtype=torch.int32, device=device) # remember to add dtype and device
                for j in range(srch.shape[0]):
                    idx[j] = srch[j][2]  # flattened locations at dim=2
                # feature map for batch k, shape[256, 512*512] with channel and index. 
                feat_tmp = feat_flt[k]  
                # select channels with given index: feat_select
                feat_select = torch.index_select(feat_tmp, dim = 1, index = idx) 
                # 1. take average and normalize to 1 dimension
                feat_mean = reduce(feat_select, 'a b -> a', reduction='mean')   
                # 2. K-means aggregation to select 1 representative feature
                # 3. supervised select boundary pixels
                # store into memory buffer
                if len(idx) == 0: # no matching class for i
                    continue
                else:
                    cls_cache[i] = feat_mean
            # after storing features in cls_cache [2, 19, 256]
            # Ema update to buffer
            self.l_buffer = self.l_buffer.to(device)
            self.f_buffer = self.f_buffer.to(device)
            self.l_buffer.detach()
            self.f_buffer.detach()

            label_expand = self.l_buffer.expand(256, self.buffer_size)
            label_expand = rearrange(label_expand, 'c n-> n c') # reshape to [K, 256] 
            cash_in = cls_cache.gather(0, label_expand) # cache_in [k, 256]
            # ema update buffer
            self.f_buffer = self.momentum * self.f_buffer + (1-self.momentum) * cash_in
            self.l_buffer = self.l_buffer.to('cpu')
            self.f_buffer = self.f_buffer.to('cpu')

    @torch.no_grad()
    def is_missing(self):
        rare_seq = [18, 12, 17, 16, 7, 6, 11, 15, 4, 5, 14, 3, 9, 13, 8, 1, 10, 2, 0]
        missing_seq = []
        for cls in rare_seq:
            if cls not in self.l_buffer: 
                missing_seq.append(cls)
        return missing_seq

    @torch.no_grad()
    def is_equally_distrubuted(self):
        rare_seq = [18, 12, 17, 16, 7, 6, 11, 15, 4, 5, 14, 3, 9, 13, 8, 1, 10, 2, 0]
        unequal_seq = []
        l_buffer = self.l_buffer.tolist()
        for cls in rare_seq:
            if l_buffer.count(cls) < ((self.buffer_size)/len(rare_seq)-1):
                unequal_seq.append(cls)
        return unequal_seq  

    @torch.no_grad()
    def update_buffer(self, feat, cls):
        """update the memory buffera and prepare for
        further sampling implementation"""
        # self.f_buffer = self.f_buffer.to(device)
        if self.buffer_idx < self.buffer_size:
            self.f_buffer[self.buffer_idx] = feat
            self.l_buffer[self.buffer_idx] = cls
            self.buffer_idx += 1
        else:
            return

    @torch.no_grad()
    def update_buffer_further(self, feat, cls):
        """update the memory buffera and prepare for
        further sampling implementation"""
        if self.update_idx < self.buffer_size:
            self.f_buffer[self.update_idx] = feat
            self.l_buffer[self.update_idx] = cls
            self.update_idx += 1
        else:
            self.update_idx = 0  # reset new pointer
            return



    @torch.no_grad()
    def cal_instance_similarity_w(self, emb_tw):
        """calculate instance similarity
        with labeled souce domain embeddings stored in the buffer"""
        dev = emb_tw.device
        # transpose them into 2 x 512 x 512 x 256
        emb_tw = rearrange(emb_tw, 'b c h w -> b h w c')
        # transpose buffer into 256 x K
        self.f_buffer = rearrange(self.f_buffer , 'a b  -> b a')
        self.f_buffer = self.f_buffer.to(dev)
        self.f_buffer.detach()
        # matrix dot product to calculate similarity(no need to select batch)
        sim_w = emb_tw @ self.f_buffer
        self.f_buffer = rearrange(self.f_buffer , 'b a  -> a b')
        self.f_buffer = self.f_buffer.to('cpu')
        # sim_w /= torch.sum(sim_w, dim=3, keepdim=True)
        sim_w = rearrange(sim_w, 'b h w c  -> b c h w')
        if self.a_smooth < 1 or self.if_calibrate == 0:
            sim_w /= torch.sum(sim_w, dim=1, keepdim=True) 
           
        del emb_tw
        return sim_w

    def cal_instance_similarity_s(self, emb_ts):
        """calculate instance similarity
        with labeled souce domain embeddings stored in the buffer"""
        dev = emb_ts.device
        # transpose them into 2 x 512 x 512 x 256
        emb_ts = rearrange(emb_ts, 'b c h w -> b h w c')
        # transpose buffer into 256 x K
        self.f_buffer = rearrange(self.f_buffer , 'a b  -> b a')
        self.f_buffer = self.f_buffer.to(dev)
        self.f_buffer.detach()
        # matrix dot product to calculate similarity(no need to select batch)
        sim_s = emb_ts @ self.f_buffer
        # transpose back
        self.f_buffer = rearrange(self.f_buffer , 'b a  -> a b')
        self.f_buffer = self.f_buffer.to('cpu')
        # transpose back to original 
        sim_s = rearrange(sim_s, 'b h w c  -> b c h w') # torch.Size([2, K, 512, 512])
        # softmax on K
        # sim_s = torch.softmax(sim_s / self.st, dim=1)
        sim_s /= torch.sum(sim_s, dim=1, keepdim=True)
        # del temp variables
        del emb_ts
        return sim_s

    def instance_similarity_loss(self, sim_w, sim_s):
        # sim_w.shape: torch.Size([2, K, 512, 512]); K is buffer size
        losses = dict()
        # calculate instance similarity loss -- L_in
        # sim_s is the predicted logits, sim_w is instance pseudo label
        # add near_0 to avoid NaN
        near_0 = 1e-10
        losses['loss_in'] = torch.sum(-sim_w.detach() * torch.log(sim_s + near_0), dim=1) # cross entropy loss on K
        losses['loss_in'] = losses['loss_in'].mean() * self.ins_lambda
        # free GPU space
        del sim_w, sim_s
        return losses
    
    # Main alteration in this part
    # We first apply weak augmentation for target_img in this fuction
    def forward_train(self, img, img_metas, gt_semantic_seg, target_img,
                      target_img_metas):
        """Forward function for training.

        Args:
            img (Tensor): Input images.
            img_metas (list[dict]): List of image info dict where each dict
                has: 'img_shape', 'scale_factor', 'flip', and may also contain
                'filename', 'ori_shape', 'pad_shape', and 'img_norm_cfg'.
                For details on the values of these keys see
                `mmseg/datasets/pipelines/formatting.py:Collect`.
            gt_semantic_seg (Tensor): Semantic segmentation masks
                used if the architecture supports semantic segmentation task.

        Returns:
            dict[str, Tensor]: a dictionary of loss components
        """
        log_vars = {}
        batch_size = img.shape[0]
        tar_batch_size = target_img.shape[0]
        dev = img.device

        """reset buffer pointer self.buffer_idx"""
        if self.buffer_idx >= self.buffer_size: # when buffer is full
            if self.local_iter % self.update_interval == 0:       # update at every interval iter
                self.if_update = 1
            else:
                self.if_update = 0

        "stop normal update when the bank is equally distributed"
        # if self.local_iter > self.stop_update:
        # if len(self.is_equally_distrubuted()) == 0:
        #     self.if_update = 0

        # Init/update ema model
        if self.local_iter == 0:
            self._init_ema_weights()
            # assert _params_equal(self.get_ema_model(), self.get_model())

        if self.local_iter > 0:
            self._update_ema(self.local_iter)
            # assert not _params_equal(self.get_ema_model(), self.get_model())
            # assert self.get_ema_model().training

        # Strong augmentation parameters
        means, stds = get_mean_std(img_metas, dev)
        strong_parameters = {
            'mix': None,
            'color_jitter': random.uniform(0, 1),
            'color_jitter_s': self.color_jitter_s,
            'color_jitter_p': self.color_jitter_p,
            'blur': random.uniform(0, 1) if self.blur else 0,
            'mean': means[0].unsqueeze(0),  # assume same normalization
            'std': stds[0].unsqueeze(0)
        }

        # Weak augmentation parameters
        means_w, stds_w = get_mean_std(target_img_metas, dev)
        weak_parameters = {
            'mix': None,
            'color_jitter': random.uniform(0, 1),
            'color_jitter_s': self.color_jitter_s,
            'color_jitter_p': self.color_jitter_p,
            'blur': random.uniform(0, 1) if self.blur else 0,
            'mean': means_w[0].unsqueeze(0),  # assume same normalization
            'std': stds_w[0].unsqueeze(0)
        }   
        

        # Train on source images --> loss L_S
        clean_losses = self.get_model().forward_train(
            img, img_metas, gt_semantic_seg, return_feat=True)
        src_feat = clean_losses.pop('features')
        
        # Obtaining source image feature embedding_s(source): torch.Size([2, 256, 512, 512]) 
        embedding_s = clean_losses.pop('embeddings')

        # Obtaining source domain seg_logit_s(source): torch size [2, 19, 512, 512]
        clean_losses.pop('seg_logits')

        if self.if_update:
            self.ema_update_bank(embedding_s, gt_semantic_seg, device=dev)


        clean_loss, clean_log_vars = self._parse_losses(clean_losses)
        log_vars.update(clean_log_vars)
        clean_loss.backward(retain_graph=self.enable_fdist)
        if self.print_grad_magnitude:
            params = self.get_model().backbone.parameters()
            seg_grads = [
                p.grad.detach().clone() for p in params if p.grad is not None
            ]
            grad_mag = calc_grad_magnitude(seg_grads)
            mmcv.print_log(f'Seg. Grad.: {grad_mag}', 'mmseg')


        # ImageNet feature distance --> loss L_FD
        if self.enable_fdist:
            feat_loss, feat_log = self.calc_feat_dist(img, gt_semantic_seg,
                                                      src_feat)
            feat_loss.backward()
            log_vars.update(add_prefix(feat_log, 'src'))
            if self.print_grad_magnitude:
                params = self.get_model().backbone.parameters()
                fd_grads = [
                    p.grad.detach() for p in params if p.grad is not None
                ]
                fd_grads = [g2 - g1 for g1, g2 in zip(seg_grads, fd_grads)]
                grad_mag = calc_grad_magnitude(fd_grads)
                mmcv.print_log(f'Fdist Grad.: {grad_mag}', 'mmseg')
        
        # Applying weak augmentation to target images before generating pseudo labels --> w_target_img
        w_target_img = target_img
        w_target_img_metas = target_img_metas
        for i in range(tar_batch_size):
            # Apply gaussian blur
            w_target_img[i] = gaussian_blur_tar(blur = weak_parameters['blur'], data = target_img[i])
            # target_img[i] = gaussian_blur_tar(blur = weak_parameters['blur'], data = target_img[i])
            # Apply color jitter on Gussian blurred w_target_img
            w_target_img[i] = color_jitter_tar(
                color_jitter=weak_parameters['color_jitter'],
                s=weak_parameters['color_jitter_s'],
                p=weak_parameters['color_jitter_p'],
                mean=weak_parameters['mean'],
                std=weak_parameters['std'],
                data=w_target_img[i])

        
        # Generate semantic pseudo-label 
        for m in self.get_ema_model().modules():
            if isinstance(m, _DropoutNd):
                m.training = False
            if isinstance(m, DropPath):
                m.training = False
        """obtaining weakly augmented target domain feature embeddings embedding_tw 
        and ema_logits """
        embedding_tw, ema_logits = self.get_ema_model().encode_decode( # with weakly augmented target images
            w_target_img, w_target_img_metas)
        # ema_logits: torch.Size([2, 19, 512, 512]) is the psudo label after encode_decode

        in_sim_w = self.cal_instance_similarity_w(embedding_tw)
        ema_softmax = torch.softmax(ema_logits.detach(), dim=1) # [2, 19, 512, 512], dim 1 is the class channel
        
        """in_sim_w calibrate with semantic similarity -> fuse_sim_w"""
        
        if self.buffer_idx >= self.buffer_size and self.if_calibrate:
            with torch.no_grad():
                self.l_buffer = self.l_buffer.to(dev)
                self.l_buffer.detach()
                label_expand = self.l_buffer.expand(2, 512, 512, self.buffer_size) # expand to [2, 512, 512, K]
                label_expand = rearrange(label_expand, 'b h w c -> b c h w') # reshape to [2, K, 512, 512]
                
                # instance aggregate
                if self.a_smooth < 1:
                    """aggregate in_sim_w from K to 19"""
                    sim_w_agg = ema_softmax.scatter_add(1, label_expand, in_sim_w) # out.scatter_add(dim, idx, src)
                    # smoothing with sim_w_agg
                    """aggregate instance pseudo labels with semantic ones"""
                    ema_softmax = ema_softmax * self.a_smooth + sim_w_agg * (1-self.a_smooth)    
                
        # ema_softmax = torch.softmax(ema_logits.detach(), dim=1) # [2, 19, 512, 512], dim 1 is the class channel
        pseudo_prob, pseudo_label = torch.max(ema_softmax, dim=1)
        ps_large_p = pseudo_prob.ge(self.pseudo_threshold).long() == 1
        ps_size = np.size(np.array(pseudo_label.cpu()))
        pseudo_weight = torch.sum(ps_large_p).item() / ps_size
        pseudo_weight = pseudo_weight * torch.ones(
            pseudo_prob.shape, device=dev)

        if self.psweight_ignore_top > 0:
            # Don't trust pseudo-labels in regions with potential
            # rectification artifacts. This can lead to a pseudo-label
            # drift from sky towards building or traffic light.
            pseudo_weight[:, :self.psweight_ignore_top, :] = 0
        if self.psweight_ignore_bottom > 0:
            pseudo_weight[:, -self.psweight_ignore_bottom:, :] = 0
        gt_pixel_weight = torch.ones((pseudo_weight.shape), device=dev)

        # Apply mixing, following DACS
        mixed_img, mixed_lbl = [None] * batch_size, [None] * batch_size
        mix_masks = get_class_masks(gt_semantic_seg)

        for i in range(batch_size):
            strong_parameters['mix'] = mix_masks[i]
            mixed_img[i], mixed_lbl[i] = strong_transform(
                strong_parameters,
                data=torch.stack((img[i], target_img[i])),
                target=torch.stack((gt_semantic_seg[i][0], pseudo_label[i])))
            _, pseudo_weight[i] = strong_transform(
                strong_parameters,
                target=torch.stack((gt_pixel_weight[i], pseudo_weight[i])))
        mixed_img = torch.cat(mixed_img)
        mixed_lbl = torch.cat(mixed_lbl)

        # Train on mixed images --> loss L_T
        mix_losses = self.get_model().forward_train(
            mixed_img, img_metas, mixed_lbl, pseudo_weight, return_feat=True) # why is gt_semantic_seg missing here: mixed_lbl contains it
        mix_losses.pop('features')
        seg_logit_s =  mix_losses.pop('seg_logits')

        if self.buffer_idx >= self.buffer_size and self.if_calibrate:
            "Unfold semantic seg_logits when buffer is full"
            with torch.no_grad():
                """unfold seg_logits from 19 -> seg_logit_uf(unfold)"""
                seg_logit_uf = seg_logit_s.gather(1, label_expand) # costs large memory

                self.l_buffer = self.l_buffer.to('cpu')
                # regenerate in_sim_w as fuse_sim_w [2, K, 512, 512]
                fuse_sim_w = in_sim_w * seg_logit_uf
                # normalize on K
                fuse_sim_w /= torch.sum(fuse_sim_w, dim=1, keepdim=True) 

            del label_expand

        embedding_ts = mix_losses.pop('embeddings')
        mix_losses = add_prefix(mix_losses, 'mix')
        mix_loss, mix_log_vars = self._parse_losses(mix_losses)
        log_vars.update(mix_log_vars)
        mix_loss.backward(retain_graph=True)

        """Calculate instance level similarity and Loss L_in
            with embedding_tw from weakly augmented view,
            embedding_ts from strongly augmented view,
            and labeled embedding_s from source domain data in the memory buffer
        """
        # Generate instance pseudo-label --> in_pseudo = in_sim_w
        # obtain in_sim_w, in_sim_s to calculate loss
        """obtaining strongly augmented target domain feature embeddings embedding_ts """
        in_sim_s = self.cal_instance_similarity_s(embedding_ts)
        # in_sim_s already normalized
        # in_sim_w.shape: [2, K, 512, 512]

        """calculate loss_in with sim_w and sim_s"""
        if self.buffer_idx >= self.buffer_size and self.if_calibrate:
            instance_losses = self.instance_similarity_loss(fuse_sim_w, in_sim_s)
        else:
            # in_sim_w /= torch.sum(in_sim_w, dim=1, keepdim=True) 
            instance_losses = self.instance_similarity_loss(in_sim_w, in_sim_s)

        # instance_losses = instance_losses * self.ins_lambda
        instance_losses = add_prefix(instance_losses, 'instance')
        in_loss, in_log_vars = self._parse_losses(instance_losses)
        log_vars.update(in_log_vars)
        
        # delete tmp variables
        del in_sim_w, in_sim_s
        #! in_loss = in_loss * self.ins_lambda
        in_loss.backward()
        # mix1_loss = mix_loss + in_loss
        # mix1_loss.backward()

        """ delete feature maps to free GPU space"""
        del embedding_s, embedding_ts, embedding_tw

        if self.local_iter % self.debug_img_interval == 0:
            out_dir = os.path.join(self.train_cfg['work_dir'],
                                   'class_mix_debug')
            os.makedirs(out_dir, exist_ok=True)
            vis_img = torch.clamp(denorm(img, means, stds), 0, 1)
            vis_trg_img = torch.clamp(denorm(target_img, means, stds), 0, 1)
            vis_mixed_img = torch.clamp(denorm(mixed_img, means, stds), 0, 1)
            for j in range(batch_size):
                rows, cols = 2, 5
                fig, axs = plt.subplots(
                    rows,
                    cols,
                    figsize=(3 * cols, 3 * rows),
                    gridspec_kw={
                        'hspace': 0.1,
                        'wspace': 0,
                        'top': 0.95,
                        'bottom': 0,
                        'right': 1,
                        'left': 0
                    },
                )
                subplotimg(axs[0][0], vis_img[j], 'Source Image')
                subplotimg(axs[1][0], vis_trg_img[j], 'Target Image')
                subplotimg(
                    axs[0][1],
                    gt_semantic_seg[j],
                    'Source Seg GT',
                    cmap='cityscapes')
                subplotimg(
                    axs[1][1],
                    pseudo_label[j],
                    'Target Seg (Pseudo) GT',
                    cmap='cityscapes')
                subplotimg(axs[0][2], vis_mixed_img[j], 'Mixed Image')
                subplotimg(
                    axs[1][2], mix_masks[j][0], 'Domain Mask', cmap='gray')
                # subplotimg(axs[0][3], pred_u_s[j], "Seg Pred",
                #            cmap="cityscapes")
                subplotimg(
                    axs[1][3], mixed_lbl[j], 'Seg Targ', cmap='cityscapes')
                subplotimg(
                    axs[0][3], pseudo_weight[j], 'Pseudo W.', vmin=0, vmax=1)
                if self.debug_fdist_mask is not None:
                    subplotimg(
                        axs[0][4],
                        self.debug_fdist_mask[j][0],
                        'FDist Mask',
                        cmap='gray')
                if self.debug_gt_rescale is not None:
                    subplotimg(
                        axs[1][4],
                        self.debug_gt_rescale[j],
                        'Scaled GT',
                        cmap='cityscapes')
                for ax in axs.flat:
                    ax.axis('off')
                plt.savefig(
                    os.path.join(out_dir,
                                 f'{(self.local_iter + 1):06d}_{j}.png'))
                plt.close()
        self.local_iter += 1

        if (self.local_iter-1) % self.update_interval == 0  and self.if_update == 1 :
            # write loss_in to csv
            path_cfg = self.train_cfg['work_dir']
            in_loss_out = os.path.join(path_cfg, 'ablation_study/loss_in')
            os.makedirs(in_loss_out, exist_ok=True)
            iter = self.local_iter
            loss_in = log_vars['instance.loss_in']
            df = pd.DataFrame({'a_name':[iter-1],'b_name':[loss_in]})
            save_loss = os.path.join(in_loss_out, 'train_in_loss.csv')
            df.to_csv(save_loss, mode = 'a', index=False, header=False)

            rare_seq1 = [18, 12, 17, 16, 7, 6, 11, 15, 4, 5, 14, 3, 9, 13, 8, 1, 10, 2, 0]
            cnt_cls = []
            cnt = 0
            for cls in rare_seq1:
                for i in self.l_buffer:
                    if cls == i:
                        cnt += 1
                cnt_cls.append(cnt)
                cnt = 0
            # write l_buffer count to csv after each update
            bank_cnt_out = os.path.join(path_cfg, 'ablation_study/label_buffer')
            os.makedirs(bank_cnt_out, exist_ok=True)
            save_bank = os.path.join(bank_cnt_out, 'bank_cnt.csv')
            df = pd.DataFrame({'a_name':[iter-1],'b_name':[cnt_cls]})
            df.to_csv(save_bank, mode = 'a', index=False, header=False)

        # saving hyper parameters:
        if self.local_iter-1 == 0:
            hyper_out = self.train_cfg['work_dir']
            path1 = os.path.join(hyper_out, 'hyper_parameters.csv')
            df1 = pd.DataFrame({'buffer_size':[self.buffer_size],
                                'update_interval':[self.update_interval], 
                                'ema_momentum':[self.momentum],
                                'BPS_threshold':[self.BPS_th],
                                'if_calibrate':[self.if_calibrate],
                                'smooth_agg':[self.a_smooth],
                                'softmax temperature':[self.wt], 
                                'lambda_ins':[self.ins_lambda],})
            df1.to_csv(path1, mode = 'a', index=False, header=True)

        # save f_buffer and l_buffer when stop updating
        if self.buffer_idx >= self.buffer_size and self.if_update == 0:
            f_bank = self.f_buffer.cpu().detach().numpy()
            l_buffer = self.l_buffer.cpu().detach().numpy()
            np.savez(r'./embedding_cache/f_buffer_s.npz', f_bank)
            np.savez(r'./embedding_cache/l_buffer_s.npz', l_buffer)
        
        return log_vars
